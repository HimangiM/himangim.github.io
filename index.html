<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Himangi Mittal</title>
  
  <meta name="author" content="Himangi Mittal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/himangi.jpg">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Himangi Mittal</name>
              </p>
              <p>I am a second year Master of Science in Robotics <a href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-robotics/">(MSR)</a> student 
                in the <a href="https://www.ri.cmu.edu/">Robotics Institute (RI)</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University (CMU)</a>, working with <a href="http://www.cs.cmu.edu/~abhinavg/">Prof. Abhinav Gupta</a> and collaborating with <a href="https://pedro-morgado.github.io/">Prof. Pedro Morgado</a> at UW-Madison. Previously, I worked as a Research Assistant at CMU with 
                <a href="https://davheld.github.io/">Prof. David Held</a> at the <a href="https://r-pad.github.io/">R-Pad Lab</a>, in collaboration with Pittsburgh-based autonomous driving company, 
                <a href="https://labs.ri.cmu.edu/argo-ai-center/">Argo AI</a>.
              </p>
              <p>
                During my Masters at CMU, I am working on self-supervised representation learning methods for multimodal audio-visual videos. Previously, as a RA at CMU, I worked on self-supervised algorithms for 3D LiDAR point clouds. I did my bachelors from
                <a href="http://www.jiit.ac.in/">Jaypee Institute of Information Technology, Noida, India</a> where I worked with 
                <a href="http://www.jiit.ac.in/dr-anuja-arora">Dr. Anuja Arora</a>.                
              </p>
              <p>
              I am currently a Teaching Assistant for the Spring 2023 course <a href="https://learning3d.github.io/">16-825: Learning for 3D Vision</a> that is being taught by <a href="https://shubhtuls.github.io/">Prof. Shubham Tulsiani</a>.
              </p>

              <p style="color:red;"><b>Looking for Internship for Summer 2023 in Computer Vision/Multi-modal Machine Learning.</b></p> 
              <p style="text-align:center">
                <a href="mailto:himangimittal@gmail.com">Email</a> &nbsp/&nbsp
                <a href="">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=n8Fc_w4AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/HimangiMittal">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/HimangiM">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/himangimittal/">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/himangi2.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/himangi2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-40px;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li> <b> Jan 2023 : Teaching Assistant </b> for 16-825: Learning for 3D Vision (being taught by Prof. Shubham Tulsiani).
               	<li> <b> Dec 2022: </b> Serving as a CVPR 2023 Reviewer (including Emergency Reviewer).
                <li> <b> Sep 2022 : </b> Paper accepted at NeurIPS 2022!!
                <li> <b> Oct 2021 : </b> Paper accepted at BMVC 2021 <font color="red">(Oral)</font>.
                <li> <b> Apr 2021 - Dec 2021: </b> I will be serving as a reviewer for ICCV 2021, AAAI 2022, WACV 2022, and CVPR 2022.
                <li> <b> Aug 2021: </b> Journal paper accepted in PAA (in collaboration with Robert Bosch, India).
                <li> <b> Feb 2021: </b> Accepted as a MSR student at CMU for Fall 2021.
                <li> <b> July 2020: </b> Presented a short paper at RSS Workshop on Self-Supervised Robot Learning 2020.
                <li> <b> Feb 2020: </b> Paper accepted at CVPR 2020 <font color="red">(Oral)</font>.
                <!-- <li> <b> Dec 2019: </b> Paper accepted at International Conference on Big Data Analytics (BDA) 2019. -->
                <!-- <li> <b> Aug 2019: </b> Started working as a Full-time Research Assistant at CMU. -->
                <!-- <li> <b> May 2018: </b> Started working as a Summer Research Intern at Robert Bosch, Bangalore, India.  -->
                <!-- <li> <b> Oct 2017: </b> Paper accepted at CODS-COMAD 2018. -->
                <!-- <li> <b> May 2017: </b> Started working as a Summer Research Intern at Indian Institute of Technology, Hyderabad, India (IIT-H). -->
              <ul>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-40px;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-20px;">
          <tbody>
          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
             <div class="one">
                <div class="two" id='neurips_image'>
                    <a href="images/cmu_logo.png"><img style="width:100%;max-width:100%" alt="" src="images/cmu_logo.png" class="hoverZoomLink"></a>
              </div>
            </div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <b>Teaching Assistant</b> for <a href="https://learning3d.github.io/">16-825: Learning for 3D Vision</a> (taught by <a href="https://shubhtuls.github.io/">Prof. Shubham Tulsiani</a>) (Spring 2023)
            </td>
          </tr>
        </tbody>
      </table>
              
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
               I am interested in devising self-supervised algorithms to minimize the need of large amounts of annotated data required for the training for 
               supervised algorithms. 
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
                <div class="two" id='neurips_image'>
                    <a href="images/neurips_final.gif"><img style="width:140%;max-width:140%" alt="" src="images/neurips_final.gif" class="hoverZoomLink"></a>
              </div>
            </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
              <papertitle>Learning State-Aware Visual Representations from Audible Interactions</papertitle>
              </a>
              <br>
              <strong>Himangi Mittal</strong>,
              <a href="https://pedro-morgado.github.io/">Pedro Morgado</a>,
              <a href='https://unnat.github.io/'>Unnat Jain</a>,
              <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>
              <br>
              <em>NeurIPS</em> 2022 <br>
              <em>ECCV 2022 Workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications</em>
              <br>
              <a href="https://arxiv.org/abs/2209.13583">Arxiv</a> /
              <a href="https://github.com/HimangiM/RepLAI">Code</a> /
              <a href="https://www.youtube.com/watch?v=hn5P8BPrPZ4&t=5s">Video</a>
              <br>
              <p></p>
              <p>We propose a self-supervised algorithm to learn representations from untrimmed, egocentric videos containing audible interactions. 
                Our method uses the audio signals in two unique ways: (1) to identify moments in time that are conducive to better self-supervised learning 
                and (2) to learn representations that focus on the visual state changes caused by audible interactions. </p>
            </td>
          </tr>
          
          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
                <div class="two" id='bmvc_image'>
                    <a href="images/arxiv_tweet10.gif"><img style="width:140%;max-width:140%" alt="profile photo" src="images/arxiv_tweet10.gif" class="hoverZoomLink"></a>
              </div>
            </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
              <papertitle>Self-Supervised Point Cloud Completion via Inpainting</papertitle>
              </a>
              <br>
              <strong>Himangi Mittal</strong>,
              <a href="https://www.ri.cmu.edu/ri-people/brian-e-okorn/">Brian Okorn</a>,
              <a href='https://www.linkedin.com/in/arpit-jangid/'>Arpit Jangid</a>,
              <a href="http://davheld.github.io/">David Held</a>
              <br>
              <em>BMVC</em> 2021 - <font color="red">Oral</font> (Selection rate 3.3%)
              <br>
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0443.pdf">Paper</a> /
              <a href="https://arxiv.org/abs/2111.10701">Arxiv</a> /
              <a href="https://github.com/HimangiM/Self-Supervised-Point-Cloud-Completion-via-Inpainting">Code</a> /
              <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0443.html">Conference Presentation</a> /
              <a href="https://self-supervised-completion-inpainting.github.io/">Webpage</a>
              <p></p>
              <p>A self-supervised method to complete the incomplete, partial point clouds for real-world settings like LiDAR where ground truth complete point cloud
              annotations are unavailable. We achieve this via inpainting where a region of the point cloud is removed and the network is trained to complete this removed region.</p>
            </td>
          </tr>
          
          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='prl_image'>
                    <a href="images/PRL-Graphical-Abstract.png"><img style="width:140%;max-width:140%" alt="profile photo" src="images/PRL-Graphical-Abstract.png" class="hoverZoomLink"></a>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/article/10.1007/s10044-021-01020-9">
              <papertitle>Harnessing emotions for depression detection</papertitle>
              </a>
              <br>
              <a href="">Sahana Prabhu Muraleedhara</a>
              <strong>Himangi Mittal</strong>,
              <a href="">Rajesh Varagani</a>,
              <a href="">Sweccha Jha</a>,
              <a href="">Shivendra Singh</a>
              <br>
              <em>Pattern Analysis and Applications Journal</em>
              <br>
              <a href="https://link.springer.com/article/10.1007/s10044-021-01020-9">Paper</a>
              <p></p>
              <p>A method for multi-modal depression detection using audio, video, and textual modalities using LSTMs. This work leverages emotions to detect an early indication of 
                depression.</p>
            </td>
          </tr>


          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr_image'>
                    <a href="images/overview_car.png"><img style="width:120%;max-width:120%" alt="profile photo" src="images/overview_car.png" class="hoverZoomLink"></a>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1912.00497.pdf">
              <papertitle>Just Go with the Flow: Self-Supervised Scene Flow Estimation</papertitle>
              </a>
              <br>
              <strong>Himangi Mittal</strong>,
              <a href="https://www.ri.cmu.edu/ri-people/brian-e-okorn/">Brian Okorn</a>,
              <a href="http://davheld.github.io/">David Held</a>
              <br>
              <em>CVPR</em> 2020 - <font color="red">Oral</font> (Selection rate 5.7%)
              <br>
              <em>RSS 2020</em> Workshop on Self-Supervised Robot Learning
              <br>
              <a href="https://arxiv.org/pdf/1912.00497.pdf">Paper</a> / 
              <a href="https://arxiv.org/abs/1912.00497">Arxiv</a> / 
              <a href="https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation">Code</a> / 
              <a href="https://just-go-with-the-flow.github.io/">Project Page</a> / 
              <a href="https://www.youtube.com/watch?v=a5HFnDPTNLk">Video</a> / 
              <a href="https://www.brainlinks-braintools.uni-freiburg.de/fileadmin/media/pdf/RSS20-SSRL/SSRL20_paper_16.pdf">Short Paper</a>
              <p></p>
              <p>A method of training scene flow that uses two self-supervised losses, based on nearest neighbors and cycle consistency. 
                These self-supervised losses allow us to train our method on large unlabeled autonomous driving datasets.</p>
            </td>
          </tr>


          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'>
                 <a href="images/Fig-7.jpg"><img style="width:120%;max-width:120%" alt="profile photo" src="images/Fig-7.jpg" class="hoverZoomLink"></a>
              </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1912.00501.pdf">
                <papertitle>Interpreting Context of Images using Scene Graphs</papertitle>
              </a>
              <br>
              <strong>Himangi Mittal</strong>,
              <a href="https://www.mirlabs.org/researchers/Ajith%20Abraham.php">Ajith Abraham</a>,
              <a href="http://www.jiit.ac.in/dr-anuja-arora">Anuja Arora</a>
              <br>
              <em>International Conference on Big Data Analytics (BDA)</em>, 2019 
              <br>
              <a href="https://dl.acm.org/doi/10.1007/978-3-030-37188-3_24">Paper</a> / 
              <a href="https://arxiv.org/pdf/1912.00501.pdf">ArXiv</a> / 
              <a href="https://github.com/HimangiM/Scene-Graph-Generation">Code</a>
              <p></p>
              <p>Predicted action and spatial relationships in images between objects detected by YOLO, then combining VGG-Net based visual features and 
                Word2Vec based semantic features.</p>
            </td>
          </tr> 

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr_image'>
                    <a href="images/anomaly.png"><img style="width:120%;max-width:120%" alt="profile photo" src="images/anomaly.png" class="hoverZoomLink"></a>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8862186">
                <papertitle>Anomaly Detection using Graph Neural Networks</papertitle>
              </a>
              <br>
              <a href="">Anshika Chaudhary</a>,
              <strong>Himangi Mittal</strong>,
              <a href="http://www.jiit.ac.in/dr-anuja-arora">Anuja Arora</a>
              <br>
              <em>International Conference on Machine Learning, Big Data, Cloud and Parallel Computing </em>, 2019
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/8862186">Paper</a> / 
              <a href="https://github.com/HimangiM/Anomaly_Graph_Neural_Net">Code</a>
              <p></p>
              <p>A method to capture the anomalous behavior in a social network based on degree, betweenness, and closeness of graph nodes using 
                Graph Neural Networks (GNN) in Keras.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr_image'>
                    <a href="images/stwalk.png"><img style="width:120%;max-width:120%" alt="profile photo" src="images/stwalk.png" class="hoverZoomLink"></a>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1711.04150.pdf">
                <papertitle>STWalk: Learning Trajectory Representations in Temporal Graphs</papertitle>
              </a>
              <br>
              <a href="https://supriya-gdptl.github.io/">Supriya Pandhre</a>,
              <strong>Himangi Mittal</strong>
              <a href="https://www.microsoft.com/en-us/research/people/gmanish/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fpeople%2Fgmanish%2F">Manish Gupta</a>,
              <a href="https://www.iith.ac.in/~vineethnb/">Vineeth N. Balasubramanian</a> <br>              
              <br>
              <em>ACM India Joint International Conference on Data Science and Management of Data (CoDS-COMAD)</em>, 2018
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3152494.3152512">Paper</a> / 
              <a href="https://arxiv.org/pdf/1711.04150.pdf">ArXiv</a> / 
              <a href="https://github.com/supriya-gdptl/STWalk">Code</a>
              <p></p>
              <p>Presents trajectory analysis of spatio-temporal graph nodes using DeepWalk algorithm in NetworkX (Python) for classification and detecting 
                changing points of interest using SVMs.</p>
            </td>
          </tr>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Service/Volunteer Work</heading>
              <ul>
                <li>Teaching Assistant for 16-825: Learning for 3D Vision (Spring 2023).
                <li><b>Reviewer Service:</b> ICCV 2021, AAAI 2022, WACV 2022, CVPR 2022, CVPR 2023 (Emergency reviewer also).
                <li>Volunteer at NeurIPS 2022 High School Outreach Program.
                <li>Mentor at CMU AI Undergraduate Mentoring Program (Fall 2022, Spring 2023).
                <li>Mentor at Spring 2023 CMU Research Mixer for undergraduate students organized by DPAC Undergraduate Research Working Group.
              </ul>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Source Code</a>                
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
