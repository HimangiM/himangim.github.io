<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Himangi Mittal</title>
  
  <meta name="author" content="Himangi Mittal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/himangi.jpg">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Himangi Mittal</name>
              </p>
              <p style="text-align:justify;">I am a second-year Ph.D. student in the <a href="https://www.ri.cmu.edu/">Robotics Institute (RI)</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University (CMU)</a>, working with <a href="https://shubhtuls.github.io/">Prof. Shubham Tulsiani</a>. 
                My research focuses on the intersection of physics-based simulation and 3D, with a particular emphasis of learning neural representations of diverse materials and their properties. 
                
                <br><br>
                I graduated with a Master of Science in Robotics <a href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-robotics/">(MSR)</a> from 
                the Robotics Institute (RI) at Carnegie Mellon University (CMU) where I worked with <a href="http://www.cs.cmu.edu/~abhinavg/">Prof. Abhinav Gupta</a> and collaborated with <a href="https://pedro-morgado.github.io/">Prof. Pedro Morgado</a> at UW-Madison. Before my Master's, I worked as a Research Assistant at CMU with 
                <a href="https://davheld.github.io/">Prof. David Held</a> at the <a href="https://r-pad.github.io/">R-Pad Lab</a>, in collaboration with Pittsburgh-based autonomous driving company, 
                <a href="https://labs.ri.cmu.edu/argo-ai-center/">Argo AI</a>.
              </p>
              <p style="text-align:justify;">
                During my Masters at CMU, I had worked on self-supervised representation learning methods for multimodal audio-visual videos and as a RA at CMU, I worked on self-supervised algorithms for 3D LiDAR point clouds. 
              </p>

              <p style="text-align:justify;">
                I have served in the organizing committee of <a href="https://sites.google.com/view/wicv-cvpr-2025">WiCV@CVPR 2025</a>, <a href="https://sites.google.com/view/wicv-cvpr-2024/home?authuser=0">WiCV@CVPR 2024</a>, <a href="https://sites.google.com/view/social-dei-cvpr-2024/">DEI Social Event</a>, and <a href="https://sites.google.com/view/cvpr2024-ecr-ai-social-event/home">Challenges/Opportunities for ECRs in Fast Paced AI Social Event!</a>
              </p>
              

              <!-- <p style="color:red;"><b>Looking for Summer 2023 Internship in Computer Vision/Multi-modal Machine Learning.</b></p>  -->
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%;text-align:center">
              <a href="images/himangi2.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/himangi2.jpg" class="hoverZoomLink"></a>
              <div style="margin-top:10px;">
                <a href="mailto:himangimittal@gmail.com">Email</a> &nbsp;/&nbsp;
                <a href="">CV</a> &nbsp;/&nbsp;
                <a href="https://scholar.google.com/citations?user=n8Fc_w4AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                <a href="https://twitter.com/HimangiMittal">Twitter</a> &nbsp;/&nbsp;
                <a href="https://github.com/HimangiM">Github</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/himangimittal/">Linkedin</a>
              </div>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-40px;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <div style="max-height:140px;overflow-y:auto;border:1px solid #e0e0e0;border-radius:6px;padding:8px 16px;background:#fafbfc;">
                <ul style="margin:0;padding-left:18px;">
                  <li> <b> Feb 2025: </b> Paper accepted at CVPR 2025.
                	<li> <b> June 2024: </b> Member of the organizing committee at <a href="https://sites.google.com/view/wicv-cvpr-2024/home?authuser=0">WiCV@CVPR 2024</a>, <a href="https://sites.google.com/view/social-dei-cvpr-2024/">DEI Social Event</a>, and <a href="https://sites.google.com/view/cvpr2024-ecr-ai-social-event/home">Challenges/Opportunities for ECRs in Fast Paced AI Social Event!</a>
                	<li> <b> Feb 2024: </b> Paper accepted at CVPR 2024.
                  <li> <b> Jan 2024 : </b> Teaching Assistant for 16-824: Visual Learning and Recognition.
                	<li> <b> August 2023 : </b> Started my Ph.D. in the Robotics Institute (RI) at Carnegie Mellon University (CMU).
                	<li> <b> May 2023 : </b> Started research internship at Honda Research Institute (HRI), San Jose, California.
                  <li> <b> Jan 2023 : </b> Teaching Assistant for 16-825: Learning for 3D Vision.
                  <li> <b> Sep 2022 : </b> Paper accepted at NeurIPS 2022.
                  <li> <b> Oct 2021 : </b> Paper accepted at BMVC 2021 <font color="red">(Oral)</font>.
                  <!-- <li> <b> Apr 2021 - Dec 2021: </b> I will be serving as a reviewer for ICCV 2021, AAAI 2022, WACV 2022, and CVPR 2022. -->
                  <li> <b> Aug 2021: </b> Journal paper accepted in PAA (in collaboration with Robert Bosch, India).
                  <li> <b> Feb 2021: </b> Accepted as a Master of Science in Robotics (MSR) student at Carnegie Mellon University for Fall 2021.
                  <li> <b> July 2020: </b> Presented a short paper at RSS Workshop on Self-Supervised Robot Learning 2020.
                  <li> <b> Feb 2020: </b> Paper accepted at CVPR 2020 <font color="red">(Oral)</font>.
                  <!-- <li> <b> Dec 2019: </b> Paper accepted at International Conference on Big Data Analytics (BDA) 2019. -->
                  <!-- <li> <b> Aug 2019: </b> Started working as a Full-time Research Assistant at CMU. -->
                  <!-- <li> <b> May 2018: </b> Started working as a Summer Research Intern at Robert Bosch, Bangalore, India.  -->
                  <!-- <li> <b> Oct 2017: </b> Paper accepted at CODS-COMAD 2018. -->
                  <!-- <li> <b> May 2017: </b> Started working as a Summer Research Intern at Indian Institute of Technology, Hyderabad, India (IIT-H). -->
                </ul>
              </div>
            </td>
          </tr>
        </tbody></table>              
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <span style="font-size:1.2em;color:#1565c0;font-weight:600;white-space:nowrap;padding:20px">Neural Physics-Based Simulation / 3D </span>
          
        <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
                <div class="two" id='cvpr_uniphy_image'>
                    <a href="images/grid_animation2.gif"><img style="width:140%;max-width:140%" alt="" src="images/grid_animation2.gif" class="hoverZoomLink"></a>
              </div>
            </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
              <papertitle>UniPhy: Learning a Unified Constitutive Model for Inverse Physics Simulation</papertitle>
              </a>
              <br>
              <strong>Himangi Mittal</strong>,
              <a href="https://payeah.net/">Peiye Zhuang</a>,
              <a href='https://hsinyinglee.com/'>Hsin-Ying Lee</a>,
              <a href="https://shubhtuls.github.io/">Shubham Tulsiani</a>
              <br>
              <em><strong>[CVPR</em> 2025]</strong><br>
              <a href="">Arxiv</a> /
              <a href="">Webpage</a> /
              <a href="">Code</a>
              <br>
              <p></p>
              <p> UniPhy is a unified latent-conditioned neural model which learns a common latent space to encode the properties of diverse materials. At inference, given motion observations for a system with unknown material parameters, UniPhy allows material inference via differentiable simulation-based latent optimization.</p>
            </td>
          </tr>	
          </tbody>
        </table>

        <br>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <span style="font-size:1.2em;color:#1565c0;font-weight:600;white-space:nowrap;padding:20px">Multi-Modal Representation Learning / Large Video-Language Models</span>
          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
                <div class="two" id='neurips_image'>
                    <a href="images/cvpr2024.gif"><img style="width:140%;max-width:140%" alt="" src="images/cvpr2024.gif" class="hoverZoomLink"></a>
              </div>
            </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
              <papertitle>Can't make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models</papertitle>
              </a>
              <br>
              <strong>Himangi Mittal</strong>,
              <a href="https://lukan94.github.io/">Nakul Agarwal</a>,
              <a href='https://shaoyuanlo.github.io/'>Shao-Yuan Lo</a>,
              <a href="https://scholar.google.com/citations?user=C6Wu8M0AAAAJ&hl=en">Kwonjoon Lee</a>
              <br>
              <em><strong>[CVPR</em> 2024]</strong><br>
              <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Mittal_Cant_Make_an_Omelette_Without_Breaking_Some_Eggs_Plausible_Action_CVPR_2024_paper.html">Paper</a> /
              <a href="https://arxiv.org/abs/2405.20305">Arxiv</a>
              <p>We leverage a large video-language model for anticipating action sequences that are plausible in the real-world. We develop the understanding of plausibility of an action sequence in a large video-language model by introducing two objective functions, a counterfactual-based plausible action sequence learning loss and a long-horizon action repetition loss. </p>
            </td>
          </tr>


          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
             <div class="one">
                <div class="two" id='neurips_image'>
                    <a href="images/neurips_final.gif"><img style="width:140%;max-width:140%" alt="" src="images/neurips_final.gif" class="hoverZoomLink"></a>
              </div>
            </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
              <papertitle>Learning State-Aware Visual Representations from Audible Interactions</papertitle>
              </a>
              <br>
              <strong>Himangi Mittal</strong>,
              <a href="https://pedro-morgado.github.io/">Pedro Morgado</a>,
              <a href='https://unnat.github.io/'>Unnat Jain</a>,
              <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a>
              <br>
              <em><strong>[NeurIPS</em> 2022]</strong><br>
              <em>ECCV 2022 Workshop on Visual Object-oriented Learning meets Interaction (VOLI): Discovery, Representations, and Applications</em><br>
              <em>Sight and Sound Workshop (CVPR 2023)</em>
              <br>
              <a href="https://arxiv.org/abs/2209.13583">Arxiv</a> /
              <a href="https://github.com/HimangiM/RepLAI">Code</a> /
              <a href="https://www.youtube.com/watch?v=hn5P8BPrPZ4&t=5s">Video</a>
              <br>
              <p></p>
              <p>We propose a self-supervised algorithm to learn representations from untrimmed, egocentric videos containing audible interactions. 
                Our method uses the audio signals in two unique ways: (1) to identify moments in time that are conducive to better self-supervised learning 
                and (2) to learn representations that focus on the visual state changes caused by audible interactions. </p>
            </td>
          </tr>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <span style="font-size:1.2em;color:#1565c0;font-weight:600;white-space:nowrap;padding:20px">Self-Supervised Learning / 3D Point Clouds</span>
          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='bmvc_image'>
                    <a href="images/arxiv_tweet10.gif"><img style="width:140%;max-width:140%" alt="profile photo" src="images/arxiv_tweet10.gif" class="hoverZoomLink"></a>
              </div>
            </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
              <papertitle>Self-Supervised Point Cloud Completion via Inpainting</papertitle>
              </a>
              <br>
              <strong>Himangi Mittal</strong>,
              <a href="https://www.ri.cmu.edu/ri-people/brian-e-okorn/">Brian Okorn</a>,
              <a href='https://www.linkedin.com/in/arpit-jangid/'>Arpit Jangid</a>,
              <a href="http://davheld.github.io/">David Held</a>
              <br>
              <em><strong>[BMVC</em> 2021 - <font color="red">Oral</font> (Selection rate 3.3%)]</strong>
              <br>
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0443.pdf">Paper</a> /
              <a href="https://arxiv.org/abs/2111.10701">Arxiv</a> /
              <a href="https://github.com/HimangiM/Self-Supervised-Point-Cloud-Completion-via-Inpainting">Code</a> /
              <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0443.html">Conference Presentation</a> /
              <a href="https://self-supervised-completion-inpainting.github.io/">Webpage</a>
              <p></p>
              <p>A self-supervised method to complete the incomplete, partial point clouds for real-world settings like LiDAR where ground truth complete point cloud
              annotations are unavailable. We achieve this via inpainting where a region of the point cloud is removed and the network is trained to complete this removed region.</p>
            </td>
          </tr>

          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr_image'>
                    <a href="images/overview_car.png"><img style="width:120%;max-width:120%" alt="profile photo" src="images/overview_car.png" class="hoverZoomLink"></a>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1912.00497.pdf">
              <papertitle>Just Go with the Flow: Self-Supervised Scene Flow Estimation</papertitle>
              </a>
              <br>
              <strong>Himangi Mittal</strong>,
              <a href="https://www.ri.cmu.edu/ri-people/brian-e-okorn/">Brian Okorn</a>,
              <a href="http://davheld.github.io/">David Held</a>
              <br>
              <em><strong>[CVPR</em> 2020 - <font color="red">Oral</font> (Selection rate 5.7%)]</strong>
              <br>
              <em>RSS 2020</em> Workshop on Self-Supervised Robot Learning
              <br>
              <a href="https://arxiv.org/pdf/1912.00497.pdf">Paper</a> / 
              <a href="https://arxiv.org/abs/1912.00497">Arxiv</a> / 
              <a href="https://github.com/HimangiM/Just-Go-with-the-Flow-Self-Supervised-Scene-Flow-Estimation">Code</a> / 
              <a href="https://www.cs.cmu.edu/news/2020/cmu-method-makes-more-data-available-training-self-driving-cars">Media article 1</a> /
              <a href="https://www.ri.cmu.edu/cmu-method-makes-more-data-available-for-training-self-driving-cars/">Media article 2</a> /
              <a href="https://just-go-with-the-flow.github.io/">Project Page</a> / 
              <a href="https://www.youtube.com/watch?v=a5HFnDPTNLk">Video</a> / 
              <a href="https://www.brainlinks-braintools.uni-freiburg.de/fileadmin/media/pdf/RSS20-SSRL/SSRL20_paper_16.pdf">Short Paper</a>
              <p></p>
              <p>A method of training scene flow that uses two self-supervised losses, based on nearest neighbors and cycle consistency. 
                These self-supervised losses allow us to train our method on large unlabeled autonomous driving datasets.</p>
            </td>
          </tr>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <span style="font-size:1.2em;color:#1565c0;font-weight:600;white-space:nowrap;padding:20px">Past Research Works (on Graphs)</span>
          


          <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mipnerf_image'>
                 <a href="images/Fig-7.jpg"><img style="width:120%;max-width:120%" alt="profile photo" src="images/Fig-7.jpg" class="hoverZoomLink"></a>
              </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1912.00501.pdf">
                <papertitle>Interpreting Context of Images using Scene Graphs</papertitle>
              </a>
              <br>
              <strong>Himangi Mittal</strong>,
              <a href="https://www.mirlabs.org/researchers/Ajith%20Abraham.php">Ajith Abraham</a>,
              <a href="http://www.jiit.ac.in/dr-anuja-arora">Anuja Arora</a>
              <br>
              <em><strong>[International Conference on Big Data Analytics (BDA)</em>, 2019]</strong>
              <br>
              <a href="https://dl.acm.org/doi/10.1007/978-3-030-37188-3_24">Paper</a> / 
              <a href="https://arxiv.org/pdf/1912.00501.pdf">ArXiv</a> / 
              <a href="https://github.com/HimangiM/Scene-Graph-Generation">Code</a>
              <p></p>
              <p>Predicted action and spatial relationships in images between objects detected by YOLO, then combining VGG-Net based visual features and 
                Word2Vec based semantic features.</p>
            </td>
          </tr> 

          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr_image'>
                    <a href="images/anomaly.png"><img style="width:120%;max-width:120%" alt="profile photo" src="images/anomaly.png" class="hoverZoomLink"></a>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/8862186">
                <papertitle>Anomaly Detection using Graph Neural Networks</papertitle>
              </a>
              <br>
              <a href="">Anshika Chaudhary</a>,
              <strong>Himangi Mittal</strong>,
              <a href="http://www.jiit.ac.in/dr-anuja-arora">Anuja Arora</a>
              <br>
              <em><strong>[International Conference on Machine Learning, Big Data, Cloud and Parallel Computing </em>, 2019]</strong>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/8862186">Paper</a> / 
              <a href="https://github.com/HimangiM/Anomaly_Graph_Neural_Net">Code</a>
              <p></p>
              <p>A method to capture the anomalous behavior in a social network based on degree, betweenness, and closeness of graph nodes using 
                Graph Neural Networks (GNN) in Keras.</p>
            </td>
          </tr>

          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='cvpr_image'>
                    <a href="images/stwalk.png"><img style="width:120%;max-width:120%" alt="profile photo" src="images/stwalk.png" class="hoverZoomLink"></a>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1711.04150.pdf">
                <papertitle>STWalk: Learning Trajectory Representations in Temporal Graphs</papertitle>
              </a>
              <br>
              <a href="https://supriya-gdptl.github.io/">Supriya Pandhre</a>,
              <strong>Himangi Mittal</strong>
              <a href="https://www.microsoft.com/en-us/research/people/gmanish/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fpeople%2Fgmanish%2F">Manish Gupta</a>,
              <a href="https://www.iith.ac.in/~vineethnb/">Vineeth N. Balasubramanian</a> <br>              
              <em><strong>[ACM India Joint International Conference on Data Science and Management of Data (CoDS-COMAD)</em>, 2018]</strong>
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3152494.3152512">Paper</a> / 
              <a href="https://arxiv.org/pdf/1711.04150.pdf">ArXiv</a> / 
              <a href="https://github.com/supriya-gdptl/STWalk">Code</a>
              <p></p>
              <p>Presents trajectory analysis of spatio-temporal graph nodes using DeepWalk algorithm in NetworkX (Python) for classification and detecting 
                changing points of interest using SVMs.</p>
            </td>
          </tr>

          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='prl_image'>
                    <a href="images/PRL-Graphical-Abstract.png"><img style="width:140%;max-width:140%" alt="profile photo" src="images/PRL-Graphical-Abstract.png" class="hoverZoomLink"></a>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://link.springer.com/article/10.1007/s10044-021-01020-9">
              <papertitle>Harnessing emotions for depression detection</papertitle>
              </a>
              <br>
              <a href="">Sahana Prabhu Muraleedhara</a>
              <strong>Himangi Mittal</strong>,
              <a href="">Rajesh Varagani</a>,
              <a href="">Sweccha Jha</a>,
              <a href="">Shivendra Singh</a>
              <br>
              <em><strong>[Pattern Analysis and Applications Journal]</strong></em>
              <br>
              <a href="https://link.springer.com/article/10.1007/s10044-021-01020-9">Paper</a>
              <p></p>
              <p>A method for multi-modal depression detection using audio, video, and textual modalities using LSTMs. This work leverages emotions to detect an early indication of 
                depression.</p>
            </td>
          </tr>
<!-- 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        	<tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Skills</heading> <br>
              <ul>
              <li> Python
              <li><a href="https://www.taichi-lang.org/">Taichi</a>
              <li> <a href="https://www.sidefx.com/">SideFX Houdini</a>       
              </ul>       
            </td>
          </tr>
        </tbody>
    	</table> -->
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        	<tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Service/Volunteer Work</heading>
              <ul>
              	<li><b>Workshop Service:</b> Member of the organizing committee at <a href="https://sites.google.com/view/wicv-cvpr-2024/home?authuser=0">WiCV@CVPR 2024</a>, <a href="https://sites.google.com/view/social-dei-cvpr-2024/">DEI Social Event</a>, and <a href="https://sites.google.com/view/cvpr2024-ecr-ai-social-event/home">Challenges/Opportunities for ECRs in Fast Paced AI Social Event!</a>
              	<li><b>Meta Reviewer Service:</b> WiCV@CVPR 2024.
                <li><b>Teaching Assistant:</b> 16-824: Visual Learning and Recognition (Spring 2024), 16-825: Learning for 3D Vision (Spring 2023).
                <li><b>Reviewer Service:</b> ICCV 2021, AAAI 2022, WACV 2022, CVPR 2022, CVPR 2023 (+ Emergency reviewer), ICCV 2023, NeurIPS 2023, Pattern Recognition Journal, WACV 2024 (+ Emergency reviewer), ACCV 2024, CVPR 2024, ICLR 2024, ICML 2024, WACV 2025, CVPR 2025, ICLR 2025, ICML 2025.
                <li>Volunteer at NeurIPS 2022 High School Outreach Program.
                <li>Mentor at CMU AI Undergraduate Mentoring Program (Fall 2022, Spring 2023, Fall 2023).
                <li>Mentor at Spring 2023 CMU Research Mixer for undergraduate students organized by DPAC Undergraduate Research Working Group.
              </ul>
            </td>
          </tr>
        </tbody>
    	</table>

    	<br>
    	<br>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-40px;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-20px;">
          <tbody>
          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle;">
             <div class="one">
                <div class="two" id='neurips_image'>
                    <a href="images/cmu_logo.png"><img style="width:100%;max-width:100%" alt="" src="images/cmu_logo.png" class="hoverZoomLink"></a>
              </div>
            </div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <b>Teaching Assistant</b> for <a href="https://visual-learning.cs.cmu.edu/index.html">16-824: Visual Learning and Recognition</a> (Spring 2024) <br>
              <b>Teaching Assistant</b> for <a href="https://learning3d.github.io/">16-825: Learning for 3D Vision</a> (Spring 2023)
            </td>
          </tr>
        </tbody>
      </table>

      


        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Source Code</a>                
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
